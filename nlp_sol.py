# -*- coding: utf-8 -*-
"""NLP_Sol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ory79T4Zb61tbUlF-clGNczs5YQfpuCd

# **NLP Exercise Solution** 
# Wiki gender pages comparison
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download("names")
nltk.download("genesis")
nltk.download("inaugural")
nltk.download("nps_chat")
nltk.download("webtext")
nltk.download("treebank")
nltk.download('gutenberg')
nltk.download('punkt')

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

"""# Israeli Vs. European Startups
## Female names mentioned in wiki pages comparison
"""

names=nltk.corpus.names.words()
female_names=nltk.corpus.names.words('female.txt')
male_names = nltk.corpus.names.words('male.txt')

"""# Analyzing Functions"""

def is_name(word):
	return True if word in names else False

def is_female_name(word):
	return True if word in female_names else False

def get_web_text(url1):
     from bs4 import BeautifulSoup
     from urllib import request
     html1 = request.urlopen(url1).read().decode('utf8')
     the_text= BeautifulSoup(html1, 'html.parser').get_text()
     return the_text

def top_names(number,text):
    txt_names=[name for name in filter(is_name,text)]
    names_freq=nltk.FreqDist(txt_names)
    top_names={}
    for name,count in names_freq.most_common(number):
        top_names[name]=count
    return top_names

def analyze_text_names(url1):
     web_text=nltk.word_tokenize(get_web_text(url1))
     all_names=[name for name in filter(is_name,web_text)]
     all_names_dict=sorted(set(all_names))
     female_names_dict=[name for name in filter(is_female_name,all_names_dict)]
    
     print("\r\n url: " + url1)
     print("\r\n percentage of female names: " + "{:.1%}".format(len(female_names_dict) / len(all_names_dict)))
     print("\r\n all names: ("+ str(len(all_names_dict))+")\r\n" + str(all_names_dict))
     print("\r\n female names: ("+str(len(female_names_dict)) +") \r\n"  + str(female_names_dict))

analyze_text_names("https://en.wikipedia.org/wiki/Silicon_Wadi")

analyze_text_names("https://en.wikipedia.org/wiki/Startup_Europe_Summit")

"""# RUSSIAN MONARCY VS. FRENCH MONARCHY
## Female names mentioned in wiki pages comparison
"""

rus_mon="https://en.wikipedia.org/wiki/List_of_Russian_monarchs"
fran_mon="https://en.wikipedia.org/wiki/List_of_French_monarchs"

rusmon_txt=get_web_text(rus_mon)
franmon_txt=get_web_text(fran_mon)

rusmon_tokenize=nltk.word_tokenize(rusmon_txt)
franmon_tokenize=nltk.word_tokenize(franmon_txt)

def text_to_data_frame_names(lst):
    female = [word for word in lst if word in female_names and word not in male_names]
    female_data = pd.DataFrame(female, index=range(len(female)), columns=['Female Names'])
    male = [word for word in lst if word in male_names]
    male_data = pd.DataFrame(male, index=range(len(male)), columns=['Male Names'])
    total_names = [word for word in lst if word in names]
    data = pd.DataFrame(total_names, index=range(len(total_names)), columns=['Names'])
    return data, male_data, female_data

def louis_combo(lst): #checks combos for Louis Catherine Ivan Peter 
    i=0
    louis_lst=[]
    roman_numerals=["I","II","III","IV","V","VI","VII","VIII","IX","X","XI","XII","XIII","XIV","XV","XVI","XVII","XVIII"]
    pseudo_name=["the","-"]
    for i in range(len(lst)):
      if lst[i]=="Louis" and (lst[i+1] in roman_numerals):
        #nick_name=(lst[i] , )
        nick_name =("Louis the "+lst[i+1])
        louis_lst.append(nick_name)
      if lst[i]=="Louis"and (lst[i+1] in pseudo_name):
        psd_nick =("Louis "+lst[i+1]+" "+lst[i+2])
        louis_lst.append(psd_nick)
      if lst[i]=="Louis-":#and (lst[i+1] in pseudo_name):
       # lng_nick =("Louis-"+lst[i+1])
        print("Louis "+lst[i+1])
      #  louis_lst.append(long_nick)
    #  elif lst[i]=="Charles":
     #   print ("Charels the: "+lst[i+1])
        i+=1
    print(len(louis_lst))
    return

louis_combo(franmon_tokenize)

rus_data, rus_male_data, rus_female_data = text_to_data_frame_names(rusmon_tokenize)
fre_data, fre_male_data, fre_female_data = text_to_data_frame_names(franmon_tokenize)

print(rus_data+" \r\n", fre_data)

print(franmon_tokenize)

franmon_top_20_names = top_names(20,franmon_tokenize)
names_fr =  franmon_top_20_names.keys()
franmon_top_20_names.pop('France')
franmon_top_20_names.pop('King')
franmon_top_20_names.pop('French')
franmon_top_20_names.pop('Son')
franmon_top_20_names.pop('April')
franmon_top_20_names.pop('May')
franmon_top_20_names.pop('June')
franmon_top_20_names.pop('August')

rusmon_top_20_names = top_names(20,rusmon_tokenize)
names_rus =  rusmon_top_20_names.keys()

rusmon_top_20_names.pop('August')
rusmon_top_20_names.pop('Prince')
rusmon_top_20_names.pop('April')
rusmon_top_20_names.pop('May')
rusmon_top_20_names.pop('June')

"""# Visualisations"""

plt.figure(figsize=(15,3))
plt.style.use('dark_background')

fig, ax = plt.subplots()

y_pos = np.arange(len(names_fr))
fr_val = franmon_top_20_names.values()

ax.barh(y_pos, fr_val, align='center',color='darkorchid')
ax.set_yticks(y_pos)
ax.set_yticklabels(names_fr)
ax.invert_yaxis()  
ax.set_xlabel('Frequancy')
ax.set_title('French Monarchs top 20')

fig, ax = plt.subplots()
y_pos = np.arange(len(names_rus))
ru_val = rusmon_top_20_names.values()

ax.barh(y_pos, ru_val, align='center',color='darksalmon')
ax.set_yticks(y_pos)
ax.set_yticklabels(names_rus)
ax.invert_yaxis()  
ax.set_xlabel('Frequancy')
ax.set_title('Russian Monarchs top 20')

male_rus = rus_male_data.nunique()[0]/rus_data.nunique()[0]
female_rus =  rus_female_data.nunique()[0]/rus_data.nunique()[0]

male_fre = fre_male_data.nunique()[0]/fre_data.nunique()[0]
female_fre =  fre_female_data.nunique()[0]/fre_data.nunique()[0]

colors = ['dodgerblue', 'pink']

plt.figure(figsize=(8,5))
plt.title("Russian Monarchy Vs. French Monarchy")
plt.bar(['Russian Monarchy','French Monarchy'],[male_rus, male_fre],color = colors[0])
plt.bar(['Russian Monarchy','French Monarchy'],[female_rus,female_fre],bottom=[male_rus,male_fre],color = colors[1])
plt.legend(['Male Names','Female Names'],bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
plt.show()



analyze_text_names("https://en.wikipedia.org/wiki/List_of_Russian_monarchs")

analyze_text_names("https://en.wikipedia.org/wiki/List_of_French_monarchs")

"""# New Analysis Section"""

#imports
from nltk.stem.snowball import FrenchStemmer #import the French stemming library
from nltk.corpus import stopwords #import stopwords from nltk corpus
import re #import the regular expressions library; will be used to strip punctuation
from collections import Counter #allows for counting the number of occurences in a list

def read_raw_file(path):
    '''reads in raw text from a text file using the argument (path), which represents the path/to/file'''
    f = open(path,"r") #open the file located at "path" as a file object (f) that is readonly
    raw = f.read().decode('utf8') # read raw text into a variable (raw) after decoding it from utf8
    f.close() #close the file now that it isn;t being used any longer
    return raw

def get_tokens(raw,encoding='utf8'):
    '''get the nltk tokens from a text'''
    tokens = nltk.word_tokenize(raw) #tokenize the raw UTF-8 text
    return tokens

def get_nltk_text(raw,encoding='utf8'):
    '''create an nltk text using the passed argument (raw) after filtering out the commas'''
    #turn the raw text into an nltk text object
    no_commas = re.sub(r'[.|,|\']',' ', raw) #filter out all the commas, periods, and appostrophes using regex
    tokens = nltk.word_tokenize(no_commas) #generate a list of tokens from the raw text
    text=nltk.Text(tokens,encoding) #create a nltk text from those tokens
    return text

def get_stopswords(type="veronis"):
    '''returns the veronis stopwords in unicode, or if any other value is passed, it returns the default nltk french stopwords'''
    if type=="veronis":
        #VERONIS STOPWORDS
        raw_stopword_list = ["Ap.", "Apr.", "GHz", "MHz", "USD", "a", "afin", "ah", "ai", "aie", "aient", "aies", "ait", "alors", "après", "as", "attendu", "au", "au-delà", "au-devant", "aucun", "aucune", "audit", "auprès", "auquel", "aura", "aurai", "auraient", "aurais", "aurait", "auras", "aurez", "auriez", "aurions", "aurons", "auront", "aussi", "autour", "autre", "autres", "autrui", "aux", "auxdites", "auxdits", "auxquelles", "auxquels", "avaient", "avais", "avait", "avant", "avec", "avez", "aviez", "avions", "avons", "ayant", "ayez", "ayons", "b", "bah", "banco", "ben", "bien", "bé", "c", "c'", "c'est", "c'était", "car", "ce", "ceci", "cela", "celle", "celle-ci", "celle-là", "celles", "celles-ci", "celles-là", "celui", "celui-ci", "celui-là", "celà", "cent", "cents", "cependant", "certain", "certaine", "certaines", "certains", "ces", "cet", "cette", "ceux", "ceux-ci", "ceux-là", "cf.", "cg", "cgr", "chacun", "chacune", "chaque", "chez", "ci", "cinq", "cinquante", "cinquante-cinq", "cinquante-deux", "cinquante-et-un", "cinquante-huit", "cinquante-neuf", "cinquante-quatre", "cinquante-sept", "cinquante-six", "cinquante-trois", "cl", "cm", "cm²", "comme", "contre", "d", "d'", "d'après", "d'un", "d'une", "dans", "de", "depuis", "derrière", "des", "desdites", "desdits", "desquelles", "desquels", "deux", "devant", "devers", "dg", "différentes", "différents", "divers", "diverses", "dix", "dix-huit", "dix-neuf", "dix-sept", "dl", "dm", "donc", "dont", "douze", "du", "dudit", "duquel", "durant", "dès", "déjà", "e", "eh", "elle", "elles", "en", "en-dehors", "encore", "enfin", "entre", "envers", "es", "est", "et", "eu", "eue", "eues", "euh", "eurent", "eus", "eusse", "eussent", "eusses", "eussiez", "eussions", "eut", "eux", "eûmes", "eût", "eûtes", "f", "fait", "fi", "flac", "fors", "furent", "fus", "fusse", "fussent", "fusses", "fussiez", "fussions", "fut", "fûmes", "fût", "fûtes", "g", "gr", "h", "ha", "han", "hein", "hem", "heu", "hg", "hl", "hm", "hm³", "holà", "hop", "hormis", "hors", "huit", "hum", "hé", "i", "ici", "il", "ils", "j", "j'", "j'ai", "j'avais", "j'étais", "jamais", "je", "jusqu'", "jusqu'au", "jusqu'aux", "jusqu'à", "jusque", "k", "kg", "km", "km²", "l", "l'", "l'autre", "l'on", "l'un", "l'une", "la", "laquelle", "le", "lequel", "les", "lesquelles", "lesquels", "leur", "leurs", "lez", "lors", "lorsqu'", "lorsque", "lui", "lès", "m", "m'", "ma", "maint", "mainte", "maintes", "maints", "mais", "malgré", "me", "mes", "mg", "mgr", "mil", "mille", "milliards", "millions", "ml", "mm", "mm²", "moi", "moins", "mon", "moyennant", "mt", "m²", "m³", "même", "mêmes", "n", "n'avait", "n'y", "ne", "neuf", "ni", "non", "nonante", "nonobstant", "nos", "notre", "nous", "nul", "nulle", "nº", "néanmoins", "o", "octante", "oh", "on", "ont", "onze", "or", "ou", "outre", "où", "p", "par", "par-delà", "parbleu", "parce", "parmi", "pas", "passé", "pendant", "personne", "peu", "plus", "plus_d'un", "plus_d'une", "plusieurs", "pour", "pourquoi", "pourtant", "pourvu", "près", "puisqu'", "puisque", "q", "qu", "qu'", "qu'elle", "qu'elles", "qu'il", "qu'ils", "qu'on", "quand", "quant", "quarante", "quarante-cinq", "quarante-deux", "quarante-et-un", "quarante-huit", "quarante-neuf", "quarante-quatre", "quarante-sept", "quarante-six", "quarante-trois", "quatorze", "quatre", "quatre-vingt", "quatre-vingt-cinq", "quatre-vingt-deux", "quatre-vingt-dix", "quatre-vingt-dix-huit", "quatre-vingt-dix-neuf", "quatre-vingt-dix-sept", "quatre-vingt-douze", "quatre-vingt-huit", "quatre-vingt-neuf", "quatre-vingt-onze", "quatre-vingt-quatorze", "quatre-vingt-quatre", "quatre-vingt-quinze", "quatre-vingt-seize", "quatre-vingt-sept", "quatre-vingt-six", "quatre-vingt-treize", "quatre-vingt-trois", "quatre-vingt-un", "quatre-vingt-une", "quatre-vingts", "que", "quel", "quelle", "quelles", "quelqu'", "quelqu'un", "quelqu'une", "quelque", "quelques", "quelques-unes", "quelques-uns", "quels", "qui", "quiconque", "quinze", "quoi", "quoiqu'", "quoique", "r", "revoici", "revoilà", "rien", "s", "s'", "sa", "sans", "sauf", "se", "seize", "selon", "sept", "septante", "sera", "serai", "seraient", "serais", "serait", "seras", "serez", "seriez", "serions", "serons", "seront", "ses", "si", "sinon", "six", "soi", "soient", "sois", "soit", "soixante", "soixante-cinq", "soixante-deux", "soixante-dix", "soixante-dix-huit", "soixante-dix-neuf", "soixante-dix-sept", "soixante-douze", "soixante-et-onze", "soixante-et-un", "soixante-et-une", "soixante-huit", "soixante-neuf", "soixante-quatorze", "soixante-quatre", "soixante-quinze", "soixante-seize", "soixante-sept", "soixante-six", "soixante-treize", "soixante-trois", "sommes", "son", "sont", "sous", "soyez", "soyons", "suis", "suite", "sur", "sus", "t", "t'", "ta", "tacatac", "tandis", "te", "tel", "telle", "telles", "tels", "tes", "toi", "ton", "toujours", "tous", "tout", "toute", "toutefois", "toutes", "treize", "trente", "trente-cinq", "trente-deux", "trente-et-un", "trente-huit", "trente-neuf", "trente-quatre", "trente-sept", "trente-six", "trente-trois", "trois", "très", "tu", "u", "un", "une", "unes", "uns", "v", "vers", "via", "vingt", "vingt-cinq", "vingt-deux", "vingt-huit", "vingt-neuf", "vingt-quatre", "vingt-sept", "vingt-six", "vingt-trois", "vis-à-vis", "voici", "voilà", "vos", "votre", "vous", "w", "x", "y", "z", "zéro", "à", "ç'", "ça", "ès", "étaient", "étais", "était", "étant", "étiez", "étions", "été", "étée", "étées", "étés", "êtes", "être", "ô"]
    else:
        #get French stopwords from the nltk kit
        raw_stopword_list = stopwords.words('french') #create a list of all French stopwords
    stopword_list = [word.decode('utf8') for word in raw_stopword_list] #make to decode the French stopwords as unicode objects rather than ascii
    return stopword_list
    

def filter_stopwords(text,stopword_list):
    '''normalizes the words by turning them all lowercase and then filters out the stopwords'''
    words=[w.lower() for w in text] #normalize the words in the text, making them all lowercase
    #filtering stopwords
    filtered_words = [] #declare an empty list to hold our filtered words
    for word in words: #iterate over all words from the text
        if word not in stopword_list and word.isalpha() and len(word) > 1: #only add words that are not in the French stopwords list, are alphabetic, and are more than 1 character
            filtered_words.append(word) #add word to filter_words list if it meets the above conditions
    filtered_words.sort() #sort filtered_words list
    return filtered_words

def stem_words(words):
    '''stems the word list using the French Stemmer'''
    #stemming words
    stemmed_words = [] #declare an empty list to hold our stemmed words
    stemmer = FrenchStemmer() #create a stemmer object in the FrenchStemmer class
    for word in words:
        stemmed_word=stemmer.stem(word) #stem the word
        stemmed_words.append(stemmed_word) #add it to our stemmed word list
    stemmed_words.sort() #sort the stemmed_words
    return stemmed_words
   
def sort_dictionary(dictionary):
    '''returns a sorted dictionary (as tuples) based on the value of each key'''
    return sorted(dictionary.items(), key=lambda x: x[1], reverse=True)

def normalize_counts(counts):
    total = sum(counts.values())
    return dict((word, float(count)/total) for word,count in counts.items())
        
def print_sorted_dictionary(tuple_dict):
    '''print the results of sort_dictionary'''
    for tup in tuple_dict:
        print (unicode(tup[1])[0:10] + '\t\t' + unicode(tup[0]))
        
def print_words(words):
    '''clean print the unicode words'''
    for word in words:
        print (word)

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

documents=[] #initialize an empty documents array
all_documents={'tokens':[],'raw':''} #initialize an empty all_documents list
xmls=franmon_tokenize
#this turns our list of documents read in from the xml files into a list of nltk documents
#each document has an index (ex. documents[0]), and within each document is a dictionary with the items: newspaper, date, raw,tokens, and 
for xml in xmls: # go through each xml document in our xmls array
    xmldoc = minidom.parse(xml_path + "\\" + xml) #parse the XML doc
    itemlist = xmldoc.getElementsByTagName(elem_name) #get all paragraph (para) elements
    newspaper = xmldoc.getElementsByTagName('newspaper') #get newspaper element
    newspaper_name= newspaper[0].attributes['name'].value #set the newspaper_name to the name attribute of the newspaper element
    date = newspaper[0].attributes['date'].value #set the newspaper date to the date attribute of the newspaper element
    raw = '' #initialize the raw variable
    for item in itemlist:
        raw += ' '.join(t.nodeValue for t in item.childNodes if t.nodeType == t.TEXT_NODE) # add text from the node's data to the variable raw if the node's data is text
    raw = re.sub(r'\s+', ' ',raw) #remove the excess whitespace from the raw text
    tokens = get_tokens(raw) #get the tokens from the text
    text = get_nltk_text(raw) #create a nltk text from the xml document's raw text
    documents.append({'newspaper':newspaper_name,'date':date,'raw':raw,'tokens':tokens,'text':text}) #add all our elements to the array (documents); each element in the array is a dictionary
    all_documents['tokens'].extend(tokens)
    all_documents['raw']+=raw
    
documents = sorted(documents, key=lambda doc: doc['date'])#sort the array according to a document's date
#to sort by paper name then date, use documents = sorted(documents, key=lambda doc: (doc['newspaper'],doc['date']))

tag_abbreviations = {
                    'A': 'adjective',
                    'Adv': 'adverb',
                    'CC': 'coordinating conjunction',
                    'Cl': 'weak clitic pronoun',
                    'CS': 'subordinating conjunction',
                    'D': 'determiner',
                    'ET': 'foreign word',
                    'I': 'interjection',
                    'NC': 'common noun',
                    'NP': 'proper noun',
                    'P': 'preposition',
                    'PREF': 'prefix',
                    'PRO': 'strong pronoun',
                    'V': 'verb',
                    'PONCT': 'punctuation mark',
                    'N': 'noun'}

#rus_mon="https://en.wikipedia.org/wiki/List_of_Russian_monarchs"
fran_mon="https://fr.wikipedia.org/wiki/Liste_des_monarques_de_France"

#rusmon_txt=get_web_text(rus_mon)
franmon_txt=get_web_text(fran_mon)

#rusmon_tokenize=nltk.word_tokenize(rusmon_txt)
franmon_tokenize=nltk.word_tokenize(franmon_txt)
print(franmon_tokenize)

#print a sorted dictionary of all stemmed and filtered words for each text
for document in franmon_tokenize:
    filtered_words = filter_stopwords(document['text'],french_stopwords)
    print ('\n',document['newspaper'],'\t',document['date'],'\n---------------')
    print_sorted_dictionary(sort_dictionary(Counter(stem_words(filtered_words))))